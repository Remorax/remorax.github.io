# Instructions to run the project

## Prerequisites

Please install the following packages before running the project:

1. Fairseq (https://github.com/facebookresearch/fairseq). Run `git checkout 31d94f556bd49bc7e61511adbda482b2c54652b5` for reproducibility.
2. Moses Decoder for preprocessing (https://github.com/moses-smt/mosesdecoder)
3. SentencePiece for tokenization (https://github.com/google/sentencepiece)
4. Trankit for lemmatization (https://github.com/nlp-uoregon/trankit)
5. The mCOLT fairseq package for training our models (https://github.com/PANXiao1994/mRASP2/tree/master/mcolt)
6. SacreBLEU v2.3.1 (https://github.com/mjpost/sacrebleu) and COMET v2.0.1 (https://github.com/Unbabel/COMET) for evaluation


## 1. Preprocessing stage

### 1.1 Word Sense Disambiguation

First, run Word Sense Disambiguation on your corpora. We experiment with two WSD systems:

- AMuSE-WSD (http://nlp.uniroma1.it/amuse-wsd/)
- ESCHER (https://github.com/SapienzaNLP/esc)

We provide the AMuSE scripts, adapted from the official documentation page (http://nlp.uniroma1.it/amuse-wsd/api-documentation) in `run_amuse.py`. This generates a pickle file for the disambiguated corpora that can be used later. 
For ESCHER, we use the scripts directly from the GitHub repository. If using this system, ensure that the disambiguated file follows the same format as that generated by the AMuSE-WSD scripts.

### 1.2. Extracting translations from BabelNet

We follow the guide in https://babelnet.org/guide to extract multilingual lexicalizations in each language of experimentation from BabelNet. We shall release these lexicalizations on the acceptance of our paper.

### 1.3 Lemmatizing MUSE lexicons

We lemmatize MUSE lexicons using Trankit (https://https://github.com/nlp-uoregon/trankit) for lemmatization. Run `python lemmatize.py <path_to_MUSE_dict>` to lemmatize the MUSE lexicons.

## 2. Pretraining

Run `bash preprocess.sh` to preprocess the corpora and generate code-switched pretraining data. This script will also launch a SLURM job with `train.sh` for training the model on the pretrained data.
For both scripts, ensure you comment out the default placeholder paths and enter appropriate directory paths of your filesystem.
